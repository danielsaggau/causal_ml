---
title: "A Gentle Introduction into Structural Causal Models"
author: 
- Daniel Saggau $\boldsymbol{\cdot}$ `daniel.saggau@campus.lmu.de`
- Department of Statistics, Ludwig Maximilian University Munich, Germany
date: " April 5th, 2021"
output: 
  pdf_document:
        number_sections: yes
        fig_caption: yes
fontsize: 12
linestretch: 1.5
citation_package: --biblatex
bibliography: [ref.bib]
nocite: '@*'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction 

Most undergraduate students that take a class in introductory statistics have heard the phrase 'correlation does not imply causation'.
While correlation implies co-occurrence, people are frequently interesting a causal understanding of relationships between variables.
One may argue that especially in high stake settings, algorithmic decision making based on co-occurrence is insufficient.
This paper tries to clarify structural differences between association-based models and causal models. 
There are different schools of thought on how to optimally model causal relationships.
Structural Causal Models and Bayesian Causal Networks are two prominent causal models.
Bayesian Causal Networks (BCN) cast a model based on conditional probabilities.
All relationships are defined in conditional probabilities.
BCNs ignore exogenous variables. 

```{tikz, notwell,  fig.cap ="Probabilistic Model",  fig.align="center", echo =F, fig.show="hold", out.width="50%" }
\usetikzlibrary{calc}
\usetikzlibrary{positioning}
\usetikzlibrary{arrows}
\begin{tikzpicture}[
    sharp corners=2pt,
    inner sep=7pt,
    node distance=3cm,
    >=latex]
\tikzstyle{my node}=[draw, shape = circle, minimum height=1cm,minimum width=1cm]
\node[my node] (A){A};
\node[my node,right =1 cm of A](C){C};
\node[my node] at ($(A)!0.5!(C)-(0pt,1.5cm)$) (T) {T};
\draw[->] (A) -- (T);
\draw[->] (A) -- (C);
\draw[->] (T) -- (C);
\end{tikzpicture}
```

In Figure 1 we can see that an example of what a probabilistic model looks like.
All variables in our graph are observable variables. 
We do not specify an exogenous variables.

Compared to the BCN, the SCM specifies the intervention distribution for data outside of our observations and specifies relationships based on equations.
Structural Causal Models (SCM) are a nonparametric modification of structural equations models with some extensions such as graphical methods and the implementation of features from the potential outcome framework.
Structural equations models (SEM) are parametric causal models which are very popular in fields like economics, psychology and sociology.
One should point out that there is a lot of controversy around SEMs because many scholars challenge the value of these parameters.
As pointed out by Pearl (2012), SEMs have been deemed useless because these parametric specifications are never correct.
The central idea of a SCM is to provide a model specifying an underlying causal process, without getting caught up in redundant parameter specifications.
To accommodate the lack of data on exogenous variables, the field of causal inference introduced tools to combine various different sources of knowledge such as theoretical knowledge.
The SCM combines central ideas from the potential outcome framework, the structural equation model and graphical models that have developed a langauge to test some assumptions that are not testable in the SEM.
Henceforth SCMs can be seen as heuristic versions of a SEM.
SCMs use assignment equations to specify an underlying data-generation-process, or in short DGP.
Note that these equations have not always had a concise mathematical notation (Pearl, 2009).
Initially there was no sign to express the assignment equation and people used the '=' and one would e.g. write A=B.
But treating an equation as a algebraic equation, an equation that is symmetric, makes no sense in the causality context.
This would imply that B=A because the order has no concrete meaning in algebraic equations.
Henceforth, the initial = sign was replaced with the ':=' which is asymmetric (Pearl, 2009).
This misconception has caused a lot of challenges which I will adddress further on in this paper. 

```{tikz,fig.cap="Structural Causal Model" ,fig.align="center", echo =F}
\usetikzlibrary{calc}
\usetikzlibrary{positioning}
\usetikzlibrary{arrows}
\usetikzlibrary{calc}
\usetikzlibrary{positioning}
\usetikzlibrary{arrows}
  \begin{tikzpicture}[
    sharp corners=2pt,
    inner sep=6pt,
    node distance=2cm,
    >=latex]
\tikzstyle{my node}=[draw, shape = circle, minimum height=1cm,minimum width=1cm]
\tikzstyle{latent}=[draw, shape = rectangle, minimum height=1cm,minimum width=1cm]
\node[my node, fill=gray!30] (A){A};
\node[latent,left of=A](UA){$U_A$};
\node[my node, fill=gray!30] at ($(A)!0.5!(A)-(0pt,1.5cm)$) (T) {T};
\node[my node,right = 1 cm of T, fill=gray!30](C){C};
\node[latent,left = 1cm of T](UT){$U_T$};
\node[latent,right = 1cm of A](UC){$U_C$};
\draw[->] (A) -- (T);
\draw[->] (UA) -- (A);
\draw[->] (UT) -- (T);
\draw[->] (UC) -- (C);
\draw[->] (A) -- (C);
\draw[->] (T) -- (C);
\end{tikzpicture}
```

In figure 2 we can see a vanilla-structural causal model. 
The square nodes represent the latent variables
The circle nodes represent the observed variables.
The arrows are our structural equations and depict the relationship between two variables.
We can immediately differentiate probabilistic and structural equation models by looking for latent variables in our directed acyclic graph.
^[Note that there are also cyclic structural causal models but no cyclic bayesian causal networks. For further information see Pearl (2009). Due to the confined scope of this paper, I will not explore cyclic structures.]

Beyond the school of thought, there is also a hierarchy of causation which Pearl introduced in 2009.
Pearl focuses on three layers namely association, intervention and counterfactuals. 
A higher level implies a deeper more detailed knowledge of the relationship between your variables.
This hierarchy examines different queries, their actions and the respective hierachical rank.
The first query is prediction where we examine based on observations.
We can see that vanilla-machine learning methods, bayesian networks and classical regression are at the lowest level in the causal hierarchy.


\footnotesize
 |Method          | Action |  Example | Usage | 
|------------------|-------------|--------------------|-------------------|
| Association $P(a|b)$               | Co-occurrence             | What happened...               |(Un-)Supervised ML, BN, Reg.  
| Intervention $P(a|do(b),c)$       | Do-manipulation           | What happens if ...            |CBN,MDP,RL    
| Counterfactual $P(a_b|a`,b`)$     | Hypotheticals   | What would have happened if...           |SCM ,PO            

Table: Pearls Hierachy of Causation (2009)

\normalsize
The second query is interventions, where we can use Pearls (2009) do-calculus to examine what happens if certain conditions holds.
Note that there are various different types of intervention, such as atomic intervention, where we set a variable to a constant, or e.g. policy intervention where we specify a different function for this specific variable.
Causal bayesian networks, markov decision processes and reinforcement learning frequent these intervention methods for e.g. policy estimation.

```{tikz,fig.cap="Policy Intervention",  fig.align= "center", echo =F}
\usetikzlibrary{calc}
\usetikzlibrary{positioning}
\usetikzlibrary{arrows}
  \begin{tikzpicture}[
    sharp corners=2pt,
    inner sep=6pt,
    node distance=2cm,
    >=latex]
\tikzstyle{my node}=[draw, shape = circle, minimum height=1cm,minimum width=1cm]
\tikzstyle{latent}=[draw, shape = rectangle, minimum height=1cm,minimum width=1cm]
\node[my node, fill=gray!30] (A){A};
\node[latent,left of=A](UA){$U_A$};
\node[my node, fill=gray!70] at ($(A)!0.5!(A)-(0pt,1.5cm)$) (T) {$T_\pi$};
\node[my node,right = 1 cm of T, fill=gray!30](C){C};
\node[latent,left = 1cm of T](UT){$U_T$};
\node[latent,right = 1cm of A](UC){$U_C$};
\draw[->] (A) -- (T);
\draw[->] (UA) -- (A);
\draw[->] (UT) -- (T);
\draw[->] (UC) -- (C);
\draw[->] (A) -- (C);
\draw[->] (T) -- (C);
\end{tikzpicture}
```


Another important factor in causal research is the understanding of time. 
Causal models mostly disregard concise notions of time and make the strong assumption that relationships between variables hold beyond the confinement of time.
Nevertheless, some research has also looked at causal models with a more concrete specification of time, treating time more like one would in a physical mechanism in hard sciences. 
In these specifications, we can use differential equations rather than assignment equations. 

Understanding statistical models beyond black-box specifications sparked the wave of interpretable machine learning.
One stream of research in interpretable machine learning is causal modelling. 
This paper addresses the linkage between standard machine learning approaches and causal machine learning. 

Existing literature has provided an excellent introduction to structural causal models and bayesian causal networks. 
Judea Pearl (2009) provided a comprehensive and accessible introduction into this topic with his book on causality.
Peters et al. (2017) added some elements, paying a lot of attention to the similarities and differences between causal modelling and physical sciences. 

This paper brings together these contributions and attempts to provide a comprehensive summary of structural causal models.
The insights in this paper are an abstraction of these pieces of work, offering the reading a basic understanding of the topic for future research and an overview of the current research.

The rest of the paper is structured as follows: 
The subsequent section examines central assumptions in causal modelling. 
The section thereafter studies the hierachy of causation, looking at the different queries in the SCM and the BCN.


# References
